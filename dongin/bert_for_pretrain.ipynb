{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98590865",
   "metadata": {},
   "source": [
    "## 참고 사이트\n",
    "https://www.ohsuz.dev/4bb36581-b0bd-49ca-acd0-530c35546009\n",
    "\n",
    "https://colab.research.google.com/drive/1VvMBT98LVJpxonUMmfpn8LGqDfUVFPS7?usp=sharing#scrollTo=BtuDt--Yb1Bx\n",
    "\n",
    "https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c22093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import time, datetime\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597786ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877aa878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_special_tokens_to_add(pair=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2e303e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9110a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/dataset_grammar.parquet\")\n",
    "#data = data.drop(index=data.index[data.Rating==2])\n",
    "#data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e030460",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent'] = data.Review.apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3502f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "for each_sent in data.sent:\n",
    "\tif len(each_sent)>1:\n",
    "\t\ttrain_list.append(each_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ded1d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['숙성 돼지고기 전문점입니다.',\n",
       " '건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다.',\n",
       " '모든 직원분들께서 전부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구워주십니다.',\n",
       " '가격 저렴한 편 아니지만 맛은 준수합니다.',\n",
       " '등심덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 않았습니다.',\n",
       " '하이라이트는 된장찌개.',\n",
       " '진짜 굿입니다.',\n",
       " '버터 간장밥, 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 남겨뒀습니다.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03266fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a99725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1972b8eb913940e1ba271e665f3bae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = [[]]\n",
    "for each_review in tqdm(train_list):\n",
    "\tfor each_line in each_review:\n",
    "\t\ttokens = tokenizer.tokenize(each_line)\n",
    "\t\ttokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\t\tdocuments[-1].append(tokens)\n",
    "\tdocuments.append([])\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05fe8a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366892"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942372db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41789d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c7f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples_from_document(document, doc_index, block_size, tokenizer, short_seq_probability, nsp_probability):\n",
    "    max_num_tokens = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    target_seq_length = max_num_tokens\n",
    "    if random.random() < short_seq_probability:\n",
    "        target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "    current_chunk = []  # a buffer stored current working segments\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                tokens_b = []\n",
    "                # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                if len(current_chunk) == 1 or random.random() < nsp_probability:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                    # This should rarely go for more than one iteration for large\n",
    "                    # corpora. However, just to be careful, we try to make sure that\n",
    "                    # the random document is not the same as the document\n",
    "                    # we're processing.\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = random.randint(0, len(documents) - 1)\n",
    "                        if random_document_index != doc_index:\n",
    "                            break\n",
    "                    # 여기서 랜덤하게 선택합니다 :-)\n",
    "                    random_document = documents[random_document_index]\n",
    "                    random_start = random.randint(0, len(random_document) - 1)\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                    while True:\n",
    "                        total_length = len(tokens_a) + len(tokens_b)\n",
    "                        if total_length <= max_num_tokens:\n",
    "                            break\n",
    "                        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                        assert len(trunc_tokens) >= 1\n",
    "                        # We want to sometimes truncate from the front and sometimes from the\n",
    "                        # back to add more randomness and avoid biases.\n",
    "                        if random.random() < 0.5:\n",
    "                            del trunc_tokens[0]\n",
    "                        else:\n",
    "                            trunc_tokens.pop()\n",
    "\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                assert len(tokens_a) >= 1\n",
    "                assert len(tokens_b) >= 1\n",
    "\n",
    "                # add special tokens\n",
    "                input_ids = tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                token_type_ids = tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                \n",
    "                # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                example = {\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                    \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                }\n",
    "\n",
    "                examples.append(example)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9f9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for doc_index, document in enumerate(documents):\n",
    "    create_examples_from_document(document=document, doc_index=doc_index, block_size=256, tokenizer=tokenizer, short_seq_probability=0.1, nsp_probability=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3346cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663090"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ca80ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   101,   9461,  17138,   9096,  68833,  12310,   9665,  25934,  34907,\n",
       "          58303,  48345,    119,   8865,  29364,   9283,  37114,  20729,   9258,\n",
       "          13890,   9283,  37114,  12092,   9682,   9891,  10739,  23665,  19105,\n",
       "           9821,  14801,  11102,   9924,  54355,   9408, 119236,   9186,  15184,\n",
       "          11261,   8848,  17138,  11467,   9367,  19855,  12310,    100,    119,\n",
       "          25701,   9707,  14279,  37712,  27023, 118683,  12424,   9665,  14646,\n",
       "           8843,  96535,  11664,   9274,  15184,   9755,  87281,  21406,    117,\n",
       "           8888,  46216,   9757,  30134,   9798, 100329,  18382,  11018,   8908,\n",
       "          69592,  16323, 119085,  48345,    119,   8843,  45465,   9663, 118878,\n",
       "          11102,   9924,   9519,  25503,  28578,   9254,  10892,   9691,  15891,\n",
       "          33188,  48345,    119,   9121,  71013, 118786, 106249,  10739,   9640,\n",
       "          14871,   8938,  74311,   8908,  10739,  11261,   9353,  11261,  18392,\n",
       "           9692,   9524, 119118,  41850,   9626,  79544,   9714,  11664,   9542,\n",
       "          14153,   9663,  21406,    100,   9523, 119118, 119081,  48345,    119,\n",
       "           9952,  10739,  17342,  41620,  11018,   9099,  13890, 119245,  21789,\n",
       "            119,    102,   9299,  42769,   8996,   9645, 118912,  10530,  19105,\n",
       "           8924, 118865,  69283,  76123,   9565,  33797,   9327,   9379, 118983,\n",
       "           9266,  80046,    119,    119,    119,   9034,  29364,   8984,  21928,\n",
       "            119,   9144,   9266,  11664,   9495,  11903,    119,    119,    119,\n",
       "            119,    119,   9886,  70221,  69164,  10530,  45397,  21155,   9448,\n",
       "          12605,    119,    119,    119,   8915,    106,    106,    106,   9986,\n",
       "          14040,   8865,  81980,   8852,  41850,   9294, 119137,  10622,   9266,\n",
       "          10622,  12508,   8887, 103155,  10739,   9555,  11664,  45397,  32158,\n",
       "          21155,  10739,   9067,  70221,  11903,  14867,   8843,  30005,  11018,\n",
       "           8867,   9765,  38631,  14102,    119,   9519,  32537,   9238, 118999,\n",
       "            102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'next_sentence_label': tensor(1)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf04832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "985c1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,   9461,  17138,   9096,  68833,  12310,   9665,    103,  34907,\n",
      "          58303,  48345,    103,    103,    103,   9283,  77237,  20729,   9258,\n",
      "          13890,   9283,  37114,    103,    103,   9891,  10739,  23665,    103,\n",
      "            103,  14801,  11102,   9924,  54355,   9408, 119236,   9186,  15184,\n",
      "          11261,    103,  17138,  11467,   9367,  19855,  12310,    100,    119,\n",
      "          25701,   9707,  14279,  37712,    103, 118683,  12424,   9665,  14646,\n",
      "           8843,  96535,  11664,   9274,  15184,   9755,  87281,    103,    117,\n",
      "           8888,  46216,    103,  30134,   9798,    103,  18382,  11018,   8908,\n",
      "          69592,  16323, 119085,  48345,    119,    103,  45465,   9663, 118878,\n",
      "          11102,   9924,   9519,  25503,  28578,   9254,  10892,   9691,  15891,\n",
      "          33188,  48345,    103,   9121,    103,    103, 106249,  10739,   9640,\n",
      "          14871,   8938,  74311,   8908,  10739,    103,   9353,  11261,  18392,\n",
      "           9692,   9524,    103,    103,   9626,  79544,   9714,  11664,   9542,\n",
      "          14153,    103,  21406,    100,   9523,    103, 119081,  48345,    103,\n",
      "           9952,    103,  17342,  41620,  11018,    103,  13890, 119245,  21789,\n",
      "            119,    102,   9299,  42769,   8996,   9645, 118912,  10530,  19105,\n",
      "            103, 118865,  69283,    103,   9565,  33797,   9327,   9379, 118983,\n",
      "           9266,  80046,    119,    119,    119,    103,  29364,   8984,  21928,\n",
      "            119,    103,   9266,    103,   9495,  11903,    119,    119,    119,\n",
      "            119,    119,   9886,  70221,  69164,  10530,  65820,  21155,   9448,\n",
      "          40104,    119,    119,    119,    103,    106,    106,    106,   9986,\n",
      "          14040,   8865,  81980,    103,  41850,   9294, 119137,  10622,   9266,\n",
      "          10622,  12508,   8887, 103155,  10739,   9555,  11664,  45397,  32158,\n",
      "          21155,    103,    103,  70221,  11903,  14867,   8843,  30005,  11018,\n",
      "           8867,   9765,  38631,  14102,    119,   9519,  32537,    103, 118999,\n",
      "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'next_sentence_label': tensor([1]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,  25934,   -100,\n",
      "           -100,   -100,    119,   8865,  29364,   -100,  37114,   -100,   -100,\n",
      "           -100,   -100,   -100,  12092,   9682,   -100,   -100,   -100,  19105,\n",
      "           9821,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   8848,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,  27023,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,  21406,   -100,\n",
      "           8888,   -100,   9757,   -100,   -100, 100329,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   8843,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,  25503,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,    119,   -100,  71013, 118786,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,  11261,   -100,   -100,   -100,\n",
      "           -100,   -100, 119118,  41850,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   9663,   -100,   -100,   -100, 119118,   -100,   -100,    119,\n",
      "           -100,  10739,   -100,   -100,   -100,   9099,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           8924,   -100,   -100,  76123,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,    119,   9034,   -100,   -100,   -100,\n",
      "           -100,   9144,   -100,  11664,   -100,   -100,    119,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,  45397,   -100,   -100,\n",
      "          12605,   -100,   -100,   -100,   8915,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   8852,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,  10739,   9067,  70221,   -100,   -100,   -100,  30005,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,  32537,   9238,   -100,\n",
      "           -100]])}\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(examples[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b929a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9461,  17138,   9096,  68833,  12310,    103,  25934,  34907,\n",
      "         58303,  48345,    119,  82718,  29364,   9283,    103,    103,   9258,\n",
      "         13890,   9283,  37114,  12092,   9682,   9891,  10739,  23665,  19105,\n",
      "          9821,  14801,  11102,   9924,  54355,   9408, 119236,   9186,  15184,\n",
      "         11261,   8848,  17138,    103,   9367,  19855,  12310,    100,    103,\n",
      "         25701,   9707,  14279,  37712,  27023, 118683,  12424,   9665,    103,\n",
      "          8843,  96535,  11664,   9274,  15184,   9755,  87281,  21406,    117,\n",
      "          8888,    103,   9757,  30134,   9798, 100329,    103,  11018,   8908,\n",
      "         69592,  16323, 119085,  48345,    119,   8843,  45465,   9663, 118878,\n",
      "         11102,   9924,   9519,  25503,  28578,   9254,  10892,    103,  15891,\n",
      "           103,  48345,    103,   9121,  71013, 118786, 106249,  10739,   9640,\n",
      "         14871,   8938,  74311,   8908,  10739,  11261,   9353,  11261,  18392,\n",
      "          9692,   9524,    103,  41850,   9626,    103,   9714,  11664,   9542,\n",
      "         14153,   9663,  21406,    100,   9523,    103, 119081,    103,    119,\n",
      "          9952,  10739,  17342,  41620,  11018,   9099,  13890, 119245,  21789,\n",
      "           119,    102,   9299,  42769,   8996,   9645, 118912,  10530,  19105,\n",
      "          8924, 118865,    103,  76123,   9565,  33797,   9327,    103, 118983,\n",
      "          9266,  80046,    119,    119,    119,    103,  29364,   8984,  21928,\n",
      "           119,   9144,   9266,  11664,   9495,  11903,    119,  90169,    119,\n",
      "           119,    119,   9886,  70221,  69164,  10530,  45397,  21155,   9448,\n",
      "         12605,    119,    119,    119,   8915,    106,    106,    106,   9986,\n",
      "         14040,   8865,    103,   8852,  41850,  17204, 119137,  10622,   9266,\n",
      "         10622,  12508,   8887, 103155,  10739,   9555,  11664,  45397,  32158,\n",
      "         21155,  10739,   9067, 103107,  11903,  14867,   8843,  30005,  82187,\n",
      "          8867,   9765,  38631,  14102,    119,   9519,  32537,   9238, 118999,\n",
      "           102])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(examples[0:1])['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e013e049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a92c25a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 숙성 돼 [MASK]기 전tott점입 驥. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 [MASK]성으로 분위 [MASK] [UNK]. 모든 직원분들께서 [MASK]부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구 [MASK]주십니다. 가격 저렴한 편 아 [MASK]지만 맛은лна [MASK]합니다. 등 [MASK]덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 [MASK]고 얇게 저며 [UNK] 않았습니다. 하이 [MASK]이트는 Mil장찌 [MASK]. [SEP] [MASK]론 내 입맛에만 그럴 수도 있지만 여기에 [MASK] 비벼 [MASK]으면... 눈 [MASK] 난 [MASK]. 또 먹고 싶다.... [MASK] 튀긴 가지에 중국식 소스... 굿!! [MASK] 혹시ādi대에 갔는데 [MASK]엇을 먹을지 계 芸이 없 [MASK] 중국음 [MASK]이 당긴다면 가 [MASK]는 걸 추천한다. 아무 리뷰 [SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(examples[0:1])['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24100b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForPreTraining.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35e3cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "685e3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./review_pre/model_output', overwrite_output_dir=True, num_train_epochs=2,per_device_train_batch_size=4, save_steps=1000, save_total_limit=2, logging_steps=100)\n",
    "trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f309a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 663090\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 82888\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61131' max='82888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61131/82888 14:09:43 < 5:02:25, 1.20 it/s, Epoch 1.48/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.920200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.811300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>1.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>1.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>1.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>1.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>1.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>1.796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>1.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>1.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.775300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>1.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>1.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>1.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>1.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>1.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>1.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>1.785700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>1.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>1.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>1.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>1.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>1.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>1.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>1.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>1.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>1.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>1.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>1.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>1.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>1.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>1.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>1.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>1.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>1.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>1.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>1.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>1.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>1.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>1.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>1.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>1.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>1.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>1.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>1.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>1.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>1.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>1.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>1.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>1.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>1.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>1.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>1.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>1.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>1.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>1.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>1.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>1.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>1.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>1.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>1.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>1.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>1.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>1.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>1.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>1.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>1.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>1.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>1.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>1.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>1.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>1.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>1.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>1.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>1.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>1.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>1.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>1.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>1.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>1.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>1.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>1.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>1.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>1.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>1.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>1.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>1.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>1.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>1.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>1.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>1.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>1.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>1.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>1.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>1.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>1.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>1.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>1.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>1.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>1.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>1.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>1.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>1.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>1.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>1.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>1.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>1.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>1.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>1.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>1.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>1.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>1.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>1.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>1.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>1.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>1.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>1.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>1.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>1.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>1.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>1.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>1.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>1.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>1.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>1.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>1.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>1.607900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>1.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>1.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>1.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>1.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>1.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>1.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>1.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>1.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>1.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>1.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>1.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>1.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>1.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>1.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>1.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>1.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>1.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>1.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>1.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>1.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>1.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>1.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>1.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>1.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>1.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>1.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>1.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>1.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>1.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>1.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>1.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>1.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>1.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>1.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>1.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>1.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>1.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>1.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>1.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>1.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>1.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>1.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>1.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>1.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>1.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>1.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>1.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>1.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38900</td>\n",
       "      <td>1.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>1.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39100</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>1.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>1.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>1.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>1.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>1.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39700</td>\n",
       "      <td>1.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>1.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>1.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40100</td>\n",
       "      <td>1.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>1.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40300</td>\n",
       "      <td>1.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>1.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>1.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>1.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40700</td>\n",
       "      <td>1.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>1.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40900</td>\n",
       "      <td>1.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>1.584600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41300</td>\n",
       "      <td>1.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>1.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>1.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>1.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>1.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>1.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41900</td>\n",
       "      <td>1.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42100</td>\n",
       "      <td>1.560600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>1.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42300</td>\n",
       "      <td>1.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>1.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>1.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42700</td>\n",
       "      <td>1.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>1.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42900</td>\n",
       "      <td>1.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43100</td>\n",
       "      <td>1.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>1.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43300</td>\n",
       "      <td>1.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>1.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>1.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>1.513700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43700</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>1.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43900</td>\n",
       "      <td>1.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>1.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>1.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44300</td>\n",
       "      <td>1.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>1.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>1.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>1.526300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44700</td>\n",
       "      <td>1.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>1.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44900</td>\n",
       "      <td>1.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45100</td>\n",
       "      <td>1.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>1.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45300</td>\n",
       "      <td>1.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>1.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>1.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>1.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45700</td>\n",
       "      <td>1.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>1.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45900</td>\n",
       "      <td>1.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46100</td>\n",
       "      <td>1.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>1.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46300</td>\n",
       "      <td>1.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>1.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>1.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>1.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46700</td>\n",
       "      <td>1.485900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46900</td>\n",
       "      <td>1.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47100</td>\n",
       "      <td>1.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>1.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47300</td>\n",
       "      <td>1.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>1.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>1.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>1.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47700</td>\n",
       "      <td>1.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>1.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47900</td>\n",
       "      <td>1.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48100</td>\n",
       "      <td>1.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>1.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48300</td>\n",
       "      <td>1.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>1.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>1.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>1.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48700</td>\n",
       "      <td>1.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>1.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48900</td>\n",
       "      <td>1.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49100</td>\n",
       "      <td>1.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>1.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49300</td>\n",
       "      <td>1.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>1.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>1.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>1.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49700</td>\n",
       "      <td>1.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>1.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49900</td>\n",
       "      <td>1.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50100</td>\n",
       "      <td>1.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>1.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50300</td>\n",
       "      <td>1.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>1.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>1.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50600</td>\n",
       "      <td>1.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50700</td>\n",
       "      <td>1.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50800</td>\n",
       "      <td>1.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50900</td>\n",
       "      <td>1.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51100</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51300</td>\n",
       "      <td>1.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51400</td>\n",
       "      <td>1.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>1.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>1.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51700</td>\n",
       "      <td>1.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51800</td>\n",
       "      <td>1.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51900</td>\n",
       "      <td>1.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52100</td>\n",
       "      <td>1.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>1.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52300</td>\n",
       "      <td>1.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52400</td>\n",
       "      <td>1.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>1.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52600</td>\n",
       "      <td>1.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52700</td>\n",
       "      <td>1.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>1.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52900</td>\n",
       "      <td>1.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53100</td>\n",
       "      <td>1.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53200</td>\n",
       "      <td>1.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53300</td>\n",
       "      <td>1.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>1.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>1.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53600</td>\n",
       "      <td>1.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53700</td>\n",
       "      <td>1.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53800</td>\n",
       "      <td>1.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53900</td>\n",
       "      <td>1.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54100</td>\n",
       "      <td>1.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54200</td>\n",
       "      <td>1.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54300</td>\n",
       "      <td>1.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54400</td>\n",
       "      <td>1.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>1.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54600</td>\n",
       "      <td>1.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54700</td>\n",
       "      <td>1.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54800</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54900</td>\n",
       "      <td>1.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55100</td>\n",
       "      <td>1.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>1.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55300</td>\n",
       "      <td>1.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55400</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>1.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55600</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55700</td>\n",
       "      <td>1.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55800</td>\n",
       "      <td>1.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55900</td>\n",
       "      <td>1.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56100</td>\n",
       "      <td>1.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56200</td>\n",
       "      <td>1.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56300</td>\n",
       "      <td>1.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56400</td>\n",
       "      <td>1.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>1.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56600</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56700</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56800</td>\n",
       "      <td>1.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56900</td>\n",
       "      <td>1.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57100</td>\n",
       "      <td>1.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57200</td>\n",
       "      <td>1.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57300</td>\n",
       "      <td>1.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57400</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>1.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57600</td>\n",
       "      <td>1.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57700</td>\n",
       "      <td>1.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57800</td>\n",
       "      <td>1.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57900</td>\n",
       "      <td>1.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58100</td>\n",
       "      <td>1.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58200</td>\n",
       "      <td>1.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58300</td>\n",
       "      <td>1.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58400</td>\n",
       "      <td>1.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>1.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58600</td>\n",
       "      <td>1.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58700</td>\n",
       "      <td>1.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58800</td>\n",
       "      <td>1.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58900</td>\n",
       "      <td>1.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59100</td>\n",
       "      <td>1.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59200</td>\n",
       "      <td>1.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59300</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59400</td>\n",
       "      <td>1.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59600</td>\n",
       "      <td>1.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59700</td>\n",
       "      <td>1.454100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59800</td>\n",
       "      <td>1.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59900</td>\n",
       "      <td>1.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60100</td>\n",
       "      <td>1.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60200</td>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60300</td>\n",
       "      <td>1.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60400</td>\n",
       "      <td>1.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60600</td>\n",
       "      <td>1.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60700</td>\n",
       "      <td>1.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60800</td>\n",
       "      <td>1.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60900</td>\n",
       "      <td>1.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61100</td>\n",
       "      <td>1.420100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-1000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-1000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-1000/pytorch_model.bin\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-2000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-2000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-2000/pytorch_model.bin\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-3000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-3000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-1000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-4000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-4000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-2000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-5000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-5000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-3000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-6000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-6000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-4000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-7000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-7000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-5000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-8000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-8000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-6000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-9000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-9000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-7000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-10000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-10000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-8000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-11000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-11000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-9000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-12000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-12000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-10000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-13000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-13000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-11000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-14000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-14000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-12000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-15000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-15000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-13000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-16000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-16000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-14000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-17000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-17000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-15000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-18000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-18000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-16000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-19000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-19000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-17000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-20000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-20000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-18000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-21000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-21000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-19000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-22000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-22000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-20000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-23000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-23000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-21000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-24000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-24000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-22000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-25000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-25000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-23000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-26000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-26000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-24000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-27000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-27000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-25000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-28000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-28000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-26000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-29000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-29000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-27000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-30000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-30000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-28000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-31000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-31000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-29000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-32000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-32000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-30000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-33000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-33000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-31000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-34000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-34000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-32000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-35000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-35000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-33000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-36000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-36000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-34000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-37000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-37000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-35000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-38000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-38000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-36000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-39000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-39000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-37000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-40000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-40000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-38000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-41000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-41000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-39000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-42000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-42000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-40000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-43000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-43000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-41000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-44000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-44000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-42000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-45000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-45000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-43000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-46000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-46000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-44000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-47000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-47000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-45000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-48000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-48000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-46000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-49000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-49000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-47000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-50000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-50000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-48000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-51000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-51000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-49000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-52000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-52000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-50000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-53000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-53000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-51000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-54000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-54000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-52000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-55000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-55000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-53000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-56000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-56000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-54000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-57000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-57000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-55000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-58000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-58000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-56000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-59000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-59000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-57000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-60000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-60000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-58000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./review_pre/model_output/checkpoint-61000\n",
      "Configuration saved in ./review_pre/model_output/checkpoint-61000/config.json\n",
      "Model weights saved in ./review_pre/model_output/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [review_pre/model_output/checkpoint-59000] due to args.save_total_limit\n",
      "/home/dongin/anaconda3/envs/attention/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f8c53f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_543720/403280937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./review_pre/model_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./review_pre/model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44259c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6659004ad2638910e883307224ae22216d48c1d40318e5d7c09413d906fa99a"
  },
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "attention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
