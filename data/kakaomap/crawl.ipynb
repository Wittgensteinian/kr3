{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException,ElementNotInteractableException,StaleElementReferenceException, UnexpectedAlertPresentException\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "chromedriver_path = \"../chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chromedriver_path, options=options)\n",
    "\n",
    "region_list = []\n",
    "rating_list = []\n",
    "category_list = []\n",
    "review_list = []\n",
    "\n",
    "\n",
    "def main():\n",
    "    global driver, load_wb, review_num\n",
    "    \n",
    "    driver.implicitly_wait(4)  # 페이지가 렌더링 될 때까지 기다림\n",
    "    driver.get('https://map.kakao.com')\n",
    "    \n",
    "    # 검색할 목록\n",
    "    regions = ['성북구','중랑구','서대문구','동대문구','서초구','양천구','구로구',\n",
    "                '용산구','성동구','도봉구','중구','노원구','강서구','광진구','마포구',\n",
    "                '영등포구','강동구','금천구','강북구','강남구','은평구','종로구','동작구',\n",
    "                '관악구','송파구', '광주', '대전', '울산', '세종', '수원', '성남', '고양',\n",
    "                '용인','부천', '안양', '남양주', '화성', '평택', '의정부', '파주',\n",
    "                '김포', '군포', '광주', '이천', '양주', '구리','하남', '여주','과천',\n",
    "                '가평', '춘천', '원주', '강릉', '속초','청주', '충주', '제천', '천안', '공주',\n",
    "                '제천', '천안', '공주', '보령', '전주', '군산', '익산', '목포', '여수', '순천', \n",
    "                '포항', '경주', '김천', '창원', '진주', '통영', '제주', '서귀포시']\n",
    " \n",
    "    # regions = ['성동구', '송파구', '영등포구']\n",
    "    \n",
    "    for i, region in enumerate(regions):\n",
    "        # delay\n",
    "        if i % 4 == 0 and i !=0 :\n",
    "            sleep(5)\n",
    "        print(\"####\", i)\n",
    "        search_store(region)\n",
    "        \n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "    \n",
    "    \n",
    "def search_store(region):\n",
    "    global driver\n",
    "    \n",
    "    region=region\n",
    "    \n",
    "    search_box = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색창\n",
    "    search_box.send_keys(region + \" 음식점\")  # 검색어 입력\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    sleep(1)\n",
    "    \n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 store list 읽기\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "    store_lst = soup.select('.placelist > .PlaceItem')  # 검색된 가게 목록\n",
    "    \n",
    "    # 검색된 첫 페이지 가게 목록 크롤링하기\n",
    "    crawl_store(region, store_lst)\n",
    "    search_box.clear()\n",
    "    \n",
    "    # 우선 더보기 클릭해서 2페이지\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)  # 2페이지로 자동으로 넘어감\n",
    "        sleep(1)\n",
    "    \n",
    "        # 2 ~ 5 페이지 읽기\n",
    "        for i in range(2, 6):\n",
    "            # 페이지 넘기기\n",
    "            store_page = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            driver.find_element_by_xpath(store_page).send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            sleep(1)\n",
    "            soup = bs(html, 'html.parser')\n",
    "            store_lst = soup.select('.placelist > .PlaceItem')  # 2 ~ 5 page에 검색된 가게 목록\n",
    "            \n",
    "            crawl_store(region, store_lst)\n",
    "            \n",
    "    except ElementNotInteractableException:  # 클릭할 수 없는 경우\n",
    "        print('not found')\n",
    "    finally:\n",
    "        search_box.clear()\n",
    "        \n",
    "        \n",
    "def crawl_store(region, store_lst):\n",
    "    region=region\n",
    "    \n",
    "    while_flag = False\n",
    "    for i, store in enumerate(store_lst):\n",
    "        # 광고에 따라 index 조정\n",
    "        if i >= 3:\n",
    "            i += 1\n",
    "        \n",
    "        store_name = store.select('.head_item > .tit_name > .link_name')[0].text  # store name\n",
    "        detail_page = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        # detail_page = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        driver.find_element_by_xpath(detail_page).send_keys(Keys.ENTER)\n",
    "        driver.switch_to.window(driver.window_handles[-1])  # 최근 열린 탭으로 전환\n",
    "        sleep(1)\n",
    "        \n",
    "        print(\"####\", store_name)\n",
    "        \n",
    "        # 첫 페이지\n",
    "        crawl_review(region, store_name)\n",
    "        \n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            n_page = len(driver.find_elements_by_class_name('link_page'))  # 페이지 수 찾기\n",
    "            for i in range(n_page-1):  # 마지막 link_page는 '다음' 버튼\n",
    "                # css selector를 이용해 페이지 버튼 누르기\n",
    "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                crawl_review(region, store_name)\n",
    "                idx += 1\n",
    "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER)  # 5 페이지가 넘는 경우 '다음' 버튼 누르기\n",
    "            sleep(1)\n",
    "            crawl_review(region, store_name)  # 리뷰 추출\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "            \n",
    "        # 그 이후 페이지\n",
    "        while True:\n",
    "            idx = 4\n",
    "            try:\n",
    "                n_page = len(driver.find_elements_by_class_name('link_page'))\n",
    "                for i in range(n_page-1):\n",
    "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                    sleep(1)\n",
    "                    crawl_review(region, store_name)\n",
    "                    idx += 1\n",
    "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER)  # 10 페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
    "                sleep(1)\n",
    "                crawl_review(region, store_name)  # 리뷰 추출\n",
    "            except (NoSuchElementException, ElementNotInteractableException):\n",
    "                print(\"no review in crawling\")\n",
    "                break\n",
    "        \n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0]) # 검색 탭으로 전환\n",
    "\n",
    "def crawl_review(region, store_name):\n",
    "    try:\n",
    "        global driver, region_list, rating_list, category_list, review_list\n",
    "        store_name_csv = store_name\n",
    "        \n",
    "        ret = True\n",
    "        \n",
    "        html = driver.page_source\n",
    "        sleep(1)\n",
    "        soup = bs(html, 'html.parser')\n",
    "        driver.implicitly_wait(4)\n",
    "        # region = soup.select('.placeinfo_default > .location_detail > span')[0].text.split()[1]\n",
    "        region=region\n",
    "        category = soup.select('.location_evaluation > .txt_location')[0].text.split()[1]\n",
    "    \n",
    "        # 첫 페이지 리뷰 목록 찾기\n",
    "        review_lst = soup.select('.list_evaluation > li')\n",
    "        \n",
    "        # 리뷰가 있는 경우\n",
    "        if len(review_lst) != 0:\n",
    "            for i, reviews in enumerate(review_lst):\n",
    "                rating = reviews.select('.grade_star > em')  # 별점\n",
    "                review = reviews.select('.txt_comment > span') # 리뷰\n",
    "                \n",
    "                # region = reviews.select('.placeinfo_default > .txt_address')\n",
    "                # store_region = store.select('.info_item > .addr > p')[0].text.split()[1]  # store address\n",
    "                if len(review) != 0:\n",
    "                    if len(rating) != 0:\n",
    "                        rating = rating[0].text.replace('점', '')\n",
    "                        review = review[0].text\n",
    "                        \n",
    "                    else:\n",
    "                        rating = str('0')\n",
    "                        review = review[0].text\n",
    "                        \n",
    "                        \n",
    "                    region_list.append(region)\n",
    "                    rating_list.append(rating)\n",
    "                    category_list.append(category)\n",
    "                    review_list.append(review)\n",
    "                    \n",
    "        else:\n",
    "            print('no review in extract')\n",
    "            ret = False\n",
    "           \n",
    "        return ret\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        data = {'Region':region_list, 'Rating':rating_list, 'Category':category_list, 'Review':review_list}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('kakaomap_'+ region + '_' + store_name_csv + str(len(region_list)) +'.csv')\n",
    "        pass\n",
    "    except StaleElementReferenceException:\n",
    "        pass\n",
    "    except UnexpectedAlertPresentException:\n",
    "        pass\n",
    "           \n",
    "\n",
    "# data = {'Region':region_list, 'Rating':rating_list, 'Category':category_list, 'Review':review_list}\n",
    "# df = pd.DataFrame(data)\n",
    "# # df.to_parquet('kakaomap_crawl.parquet', engine='pyarrow')\n",
    "# df.to_csv('kakaomap_'+ region + '_' + store_name_csv + str(len(region_list)) +'.csv')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7014\n",
      "7014\n",
      "7014\n",
      "7014\n"
     ]
    }
   ],
   "source": [
    "print(len(region_list))\n",
    "print(len(rating_list))\n",
    "print(len(category_list))\n",
    "print(len(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Rating Category                                             Review\n",
      "8374     포항      4        회                       투박한 느낌의 물회. 오래 기다려야 하는 게 단점.\n",
      "8375     포항      3        회  맵지도 달지도 새콤하지않은 담백한 맛의 어촌스타일의 가자미물회~ 바싹 구워주는 가자...\n",
      "8376     포항      4        회                             자연산물회 고기못잡으면 가게장사안하는 집\n",
      "8377     포항      5        회                우리 가족들만 아는 맛집이었는데 수요미식회에 나오다니 젠장...\n",
      "8378     포항      5        회                     tvN 수요미식회 \"포항\"편에 가자미물회가 소개된 식당\n",
      "8379     포항      5        회  오리지날 포항식 물회입니다. 국물 없이 고추장 양념에 비벼먹는 까재미(가자미)회 입...\n",
      "8380     포항      1        회  보통 생각하는 물회는 아닙니다. 더운날 시원한 물회를 먹으려고 했다가 폭망했습니다....\n",
      "8381     포항      4        회          맛은 잇엇지만 시원한 국물맛을 즐기는 사람은 별로일거에요. 그래도 맛은 굿\n",
      "8382     포항      3        회                                                   \n",
      "8383     포항      3        회                   가자미회 & 구이 깔끔함 휴일은 기다리는 사람 많아서 no\n"
     ]
    }
   ],
   "source": [
    "data = {'Region':region_list, 'Rating':rating_list, 'Category':category_list, 'Review':review_list}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('kakaomap_군산_첫번째_20944.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49de8edb4a174572adfce8885130fd6486c82b1df7252289157b68155bf6ea64"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
